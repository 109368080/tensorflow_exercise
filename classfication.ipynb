{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "train_ratio= 0.9\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "x_, y_ = digits.data, digits.target\n",
    "\n",
    "x_ = x_ / x_.max() \n",
    "\n",
    "y_one_hot = np.zeros((len(y_), 10))  \n",
    "y_one_hot[np.arange(len(y_)), y_] = 1\n",
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_, \n",
    "                                                    y_one_hot, \n",
    "                                                    test_size = 0.05, \n",
    "                                                    stratify  = y_)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, \n",
    "                                                      y_train, \n",
    "                                                      test_size = 1.0 - train_ratio,\n",
    "                                                      stratify = y_train.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build low level with tensor elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(shape = (None, x_train.shape[1]),\n",
    "                       name = 'x',\n",
    "                       dtype = tf.float32)\n",
    "    y = tf.placeholder(shape = (None, y_train.shape[1]), \n",
    "                           name = 'y',\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('hidden_layer1'):\n",
    "    w1 = tf.get_variable('weight1', \n",
    "                         shape= [x_train.shape[1], 25], \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b1 = tf.get_variable('bias1', \n",
    "                         shape= [25], \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.constant_initializer(0.0))  \n",
    "    x_h1 = tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "with tf.variable_scope('hidden_layer2'):\n",
    "    w2 = tf.get_variable('weight2', \n",
    "                         shape = [25, 25], \n",
    "                         dtype = tf.float32, \n",
    "                         initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b2 = tf.get_variable('bias2', \n",
    "                         shape = [25], \n",
    "                         dtype = tf.float32, \n",
    "                         initializer = tf.constant_initializer(0.0))  \n",
    "    x_h2 = tf.nn.relu(tf.add(tf.matmul(x_h1, w2), b2))\n",
    "    \n",
    "with tf.variable_scope('hidden_layer3'):\n",
    "    w3 = tf.get_variable('weight3', \n",
    "                         shape = [25, 25], \n",
    "                         dtype =tf.float32, \n",
    "                         initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b3 = tf.get_variable('bias3', \n",
    "                         shape = [25], \n",
    "                         dtype = tf.float32, \n",
    "                         initializer = tf.constant_initializer(0.0))  \n",
    "    x_h3 = tf.nn.relu(tf.add(tf.matmul(x_h2, w3), b3))\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    w4 = tf.get_variable('weight4',\n",
    "                         shape = [25, y_train.shape[1]],\n",
    "                         dtype = tf.float32,\n",
    "                         initializer = tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b4 = tf.get_variable('bias4', \n",
    "                         shape = [y_train.shape[1]], \n",
    "                         dtype = tf.float32, \n",
    "                         initializer = tf.constant_initializer(0.0))\n",
    "    output = tf.add(tf.matmul(x_h3, w4), b4)\n",
    "    \n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = output, labels = y))\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(output), 1), tf.argmax(y, 1)) #如果答案對則回傳truth\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #將回傳的truth/false轉乘1/0並計算平均(計算正確率)\n",
    "#-----------------------optimizer---------------------------------\n",
    "with tf.name_scope('train'):\n",
    "    #使用adam做optimization最小化loss funciotn(不斷取微分並逼近local min)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in tqdm(range(epochs)):\n",
    "        iterations = int(np.floor(len(x_train) / batch_size))\n",
    "        train_loss_collector = []\n",
    "        train_acc_collector = []\n",
    "        for j in np.arange(iterations):\n",
    "            batch_idx_start = j * batch_size\n",
    "            batch_idx_stop = (j+1) * batch_size\n",
    "            \n",
    "            x_batch = x_train[batch_idx_start : batch_idx_stop] \n",
    "            y_batch = y_train[batch_idx_start : batch_idx_stop]\n",
    "            \n",
    "            this_loss, this_acc, _ = sess.run([loss, compute_acc, train_step], \n",
    "                                    feed_dict = {x : x_batch,\n",
    "                                                 y : y_batch})\n",
    "            \n",
    "            train_loss_collector.append(this_loss) #記錄每個batch的loss\n",
    "            train_acc_collector.append(this_acc)\n",
    "            \n",
    "        valid_acc, valid_loss = sess.run([compute_acc, loss],\n",
    "                                         feed_dict = {x : x_valid,\n",
    "                                                      y : y_valid})\n",
    "        valid_loss_list.append(valid_loss) #記錄validation loss\n",
    "        valid_acc_list.append(valid_acc)   #記錄validation acc\n",
    "        train_loss_list.append(np.mean(train_loss_collector)) #記錄每個epoch 平均 loss\n",
    "        train_acc_list.append(np.mean(train_acc_collector))   #記錄每個epoch 平均 acc\n",
    "\n",
    "        # at the end of each epoch, shuffle the data 重新排列資料並進入下一個i(epochs)\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "        \n",
    "    test_acc, test_loss = sess.run([compute_acc, loss],\n",
    "                                    feed_dict = {x : x_test,\n",
    "                                                 y : y_test})\n",
    "print('--- training done ---')\n",
    "print('testing accuracy: %.2f' % test_acc)\n",
    "#------------------------------------------------------plot----------------------------------------------------------\n",
    "plt.plot(np.arange(len(train_loss_list)), train_loss_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(valid_loss_list)), valid_loss_list, 'r', label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_list)), train_acc_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(valid_acc_list)), valid_acc_list, 'r', label = 'valid')\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build with \"Layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # clean graph\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(shape = (None,x_train.shape[1]), \n",
    "                             name = 'x',\n",
    "                             dtype=tf.float32)\n",
    "    y = tf.placeholder(shape = (None, y_train.shape[1]), \n",
    "                           name = 'y',\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('hidden_layer1'):\n",
    "    x_h1 = tf.layers.dense(inputs= x, units= 64, activation=tf.nn.relu)\n",
    "\n",
    "with tf.variable_scope('hidden_layer2'):\n",
    "    x_h2 = tf.layers.dense(inputs= x_h1, units= 64, activation=tf.nn.relu)\n",
    "    \n",
    "with tf.variable_scope('hidden_layer3'):\n",
    "    x_h3 = tf.layers.dense(inputs= x_h2, units= 64, activation=tf.nn.relu)\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    output = tf.layers.dense(x_h3, 10)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = output, labels = y))\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(output), 1), tf.argmax(y, 1))\n",
    "    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in tqdm(range(epochs)):\n",
    "        iterations = int(np.floor(len(x_train) / batch_size))\n",
    "        train_loss_collector = []\n",
    "        train_acc_collector = []\n",
    "        for j in np.arange(iterations):\n",
    "            batch_idx_start = j * batch_size\n",
    "            batch_idx_stop = (j+1) * batch_size\n",
    "            \n",
    "            x_batch = x_train[batch_idx_start : batch_idx_stop] \n",
    "            y_batch = y_train[batch_idx_start : batch_idx_stop]\n",
    "            \n",
    "            this_loss, this_acc, _ = sess.run([loss, compute_acc, train_step], \n",
    "                                    feed_dict = {x : x_batch,\n",
    "                                                 y : y_batch})\n",
    "            \n",
    "            train_loss_collector.append(this_loss) #記錄每個batch的loss\n",
    "            train_acc_collector.append(this_acc)\n",
    "            \n",
    "        valid_acc, valid_loss = sess.run([compute_acc, loss],\n",
    "                                         feed_dict = {x : x_valid,\n",
    "                                                      y : y_valid})\n",
    "        valid_loss_list.append(valid_loss) #記錄validation loss\n",
    "        valid_acc_list.append(valid_acc)   #記錄validation acc\n",
    "        train_loss_list.append(np.mean(train_loss_collector)) #記錄每個epoch 平均 loss\n",
    "        train_acc_list.append(np.mean(train_acc_collector))   #記錄每個epoch 平均 acc\n",
    "\n",
    "        # at the end of each epoch, shuffle the data 重新排列資料並進入下一個i(epochs)\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "        \n",
    "    test_acc, test_loss = sess.run([compute_acc, loss],\n",
    "                                    feed_dict = {x : x_test,\n",
    "                                                 y : y_test})\n",
    "print('--- training done ---')\n",
    "print('testing accuracy: %.2f' % test_acc)\n",
    "#------------------------------------------------------plot----------------------------------------------------------\n",
    "plt.plot(np.arange(len(train_loss_list)), train_loss_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(valid_loss_list)), valid_loss_list, 'r', label = 'valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_list)), train_acc_list, 'b', label = 'train')\n",
    "plt.plot(np.arange(len(valid_acc_list)), valid_acc_list, 'r', label = 'valid')\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
